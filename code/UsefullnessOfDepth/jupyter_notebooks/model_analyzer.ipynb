{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m27 11:26:47 \u001b[0mUsing Ham Decoder\n",
      "\u001b[32m27 11:26:47 \u001b[0mIniting weights ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single GPU\n",
      "0.2\n",
      "64\n",
      "spatial True\n",
      "S 1\n",
      "D 512\n",
      "R 64\n",
      "train_steps 6\n",
      "eval_steps 7\n",
      "inv_t 100\n",
      "eta 0.9\n",
      "rand_init True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m27 11:26:48 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::add_ encountered 13 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:48 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::gelu encountered 40 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:48 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::mul encountered 159 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:48 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::add encountered 135 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:48 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::rand encountered 47 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:48 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::div encountered 60 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:48 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::pow encountered 10 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:48 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::softmax encountered 11 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:48 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::linalg_vector_norm encountered 1 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:48 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::clamp_min encountered 1 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:48 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::expand_as encountered 1 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:48 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::feature_dropout encountered 1 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:48 \u001b[0m\u001b[1;31mWRN The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "backbone.stages.0.0.dropout_layer, criterion\u001b[0m\n",
      "\u001b[32m27 11:26:48 \u001b[0mUsing Ham Decoder\n",
      "\u001b[32m27 11:26:48 \u001b[0mIniting weights ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dformer tiny flops:  15412004992\n",
      "DFormer Tiny GFLOPs per image:  7.7\n",
      "single GPU\n",
      "0.2\n",
      "64\n",
      "spatial True\n",
      "S 1\n",
      "D 512\n",
      "R 64\n",
      "train_steps 6\n",
      "eval_steps 7\n",
      "inv_t 100\n",
      "eta 0.9\n",
      "rand_init True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m27 11:26:50 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::add_ encountered 13 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:50 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::gelu encountered 61 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:50 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::mul encountered 243 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:50 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::add encountered 205 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:50 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::rand encountered 75 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:50 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::div encountered 88 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:50 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::pow encountered 17 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:50 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::softmax encountered 18 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:50 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::linalg_vector_norm encountered 1 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:50 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::clamp_min encountered 1 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:50 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::expand_as encountered 1 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:50 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::feature_dropout encountered 1 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:50 \u001b[0m\u001b[1;31mWRN The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "backbone.stages.0.0.dropout_layer, criterion\u001b[0m\n",
      "\u001b[32m27 11:26:50 \u001b[0mUsing backbone: Segformer-B2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dformer large flops:  72376557984\n",
      "DFormer Large GFLOPs per image:  36.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m27 11:26:50 \u001b[0mUsing MLP Decoder\n",
      "\u001b[32m27 11:26:50 \u001b[0mIniting weights ...\n",
      "\u001b[32m27 11:26:52 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::mul encountered 132 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:52 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::softmax encountered 40 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:52 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::add encountered 92 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:52 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::gelu encountered 32 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:52 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::bernoulli_ encountered 60 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:52 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::div_ encountered 60 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:52 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::adaptive_max_pool2d encountered 4 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:52 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::sigmoid encountered 8 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:52 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::add_ encountered 9 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:52 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::feature_dropout encountered 1 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:52 \u001b[0m\u001b[1;31mWRN The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "criterion\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMX flops:  76846170240\n",
      "CMX GFLOPs per image:  38.4\n",
      "using batch norm:  <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "The pretrained file is not in the correct location\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m27 11:26:53 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::add_ encountered 141 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:53 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::pad encountered 63 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:53 \u001b[0m\u001b[1;31mWRN Unsupported operator aten::add encountered 20 time(s)\u001b[0m\n",
      "\u001b[32m27 11:26:53 \u001b[0m\u001b[1;31mWRN The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "criterion\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deeplab flops:  102340281136\n",
      "Deeplab GFLOPs per image:  51.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from models.builder import EncoderDecoder as segmodel\n",
    "from models_CMX.builder import EncoderDecoder as cmxmodel\n",
    "from model_pytorch_deeplab_xception.deeplab import DeepLab\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# code\\UsefullnessOfDepth\\configs\\SynthDet\\SynthDet_template_DFormer_Tiny.py\n",
    "config_path = r\"..\\configs\\SynthDet\\SynthDet_template_DFormer_Tiny.py\"\n",
    "config_module = importlib.import_module(\"configs.SynthDet.SynthDet_template_DFormer_Tiny\")\n",
    "config = config_module.config\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   \n",
    "\n",
    "rgb_input = torch.randn(2, 3, 400, 400).to(device)\n",
    "depth_input = torch.randn(2, 1, 400, 400).to(device)\n",
    "\n",
    "config.decoder = \"ham\"\n",
    "config.backbone = \"DFormer-Tiny\"\n",
    "dformer_tiny_model = segmodel(cfg=config).to(device)\n",
    "flops = FlopCountAnalysis(dformer_tiny_model, (rgb_input, depth_input))\n",
    "print(\"Dformer tiny flops: \", flops.total())\n",
    "print(\"DFormer Tiny GFLOPs per image: \", round(flops.total() / 2e9, 1))\n",
    "\n",
    "config.decoder = \"ham\"\n",
    "config.backbone = \"DFormer-Large\"\n",
    "config.drop_path_rate = 0.2\n",
    "dformer_large_model = segmodel(cfg=config).to(device)\n",
    "flops = FlopCountAnalysis(dformer_large_model, (rgb_input, depth_input))\n",
    "print(\"Dformer large flops: \", flops.total())\n",
    "print(\"DFormer Large GFLOPs per image: \", round(flops.total() / 2e9, 1))\n",
    "\n",
    "config.decoder = \"MLPDecoder\"\n",
    "config.backbone = \"mit_b2\"\n",
    "cmxmodel = cmxmodel(cfg=config).to(device)\n",
    "flops = FlopCountAnalysis(cmxmodel, (rgb_input, depth_input))\n",
    "print(\"CMX flops: \", flops.total())\n",
    "print(\"CMX GFLOPs per image: \", round(flops.total() / 2e9, 1))\n",
    "    \n",
    "config.backbone = \"xception\"\n",
    "deeplab_model = DeepLab(cfg=config, criterion=nn.CrossEntropyLoss(reduction=\"none\"), norm_layer=nn.BatchNorm2d).to(device)\n",
    "flops = FlopCountAnalysis(deeplab_model, rgb_input)\n",
    "print(\"Deeplab flops: \", flops.total())\n",
    "print(\"Deeplab GFLOPs per image: \", round(flops.total() / 2e9, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
